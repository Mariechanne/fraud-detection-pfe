#!/usr/bin/env python3
"""
Script d'entraÃ®nement du modÃ¨le de dÃ©tection de fraude.

Usage:
    python scripts/train_model.py --data data/raw/creditcard.csv --output models/my_model
"""

import argparse
import json
import sys
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    precision_recall_curve,
    precision_score,
    recall_score,
    roc_auc_score,
    auc,
)
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler


def load_data(data_path: Path) -> pd.DataFrame:
    """Charge les donnÃ©es."""
    print(f"ğŸ“‚ Chargement des donnÃ©es depuis {data_path}...")
    df = pd.read_csv(data_path)
    print(f"âœ… {len(df):,} transactions chargÃ©es")
    return df


def split_data(df: pd.DataFrame, test_size: float = 0.3, random_state: int = 42):
    """SÃ©pare les donnÃ©es en train/valid/test."""
    print("\nğŸ“Š SÃ©paration des donnÃ©es...")

    # SÃ©parer features et cible
    X = df.drop("Class", axis=1)
    y = df["Class"]

    # Train/temp split (70/30)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )

    # Valid/Test split (15/15)
    X_valid, X_test, y_valid, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=random_state, stratify=y_temp
    )

    print(f"   Train: {len(X_train):,} | Valid: {len(X_valid):,} | Test: {len(X_test):,}")
    print(f"   Fraudes - Train: {y_train.sum()} | Valid: {y_valid.sum()} | Test: {y_test.sum()}")

    return X_train, X_valid, X_test, y_train, y_valid, y_test


def build_pipeline(smote_strategy: float = 0.2, random_state: int = 42) -> Pipeline:
    """Construit le pipeline ML."""
    print("\nğŸ”§ Construction du pipeline...")

    # Preprocessing
    preprocessor = ColumnTransformer(
        transformers=[("scaler", StandardScaler(), ["Amount", "Time"])],
        remainder="passthrough",
    )

    # Pipeline
    pipeline = Pipeline(
        [
            ("prep", preprocessor),
            ("smote", SMOTE(sampling_strategy=smote_strategy, random_state=random_state)),
            (
                "model",
                RandomForestClassifier(
                    n_estimators=100,
                    max_depth=20,
                    min_samples_split=10,
                    random_state=random_state,
                    n_jobs=-1,
                ),
            ),
        ]
    )

    print("âœ… Pipeline crÃ©Ã©: StandardScaler â†’ SMOTE â†’ RandomForest")
    return pipeline


def train_model(pipeline: Pipeline, X_train, y_train):
    """EntraÃ®ne le modÃ¨le."""
    print("\nğŸ¯ EntraÃ®nement du modÃ¨le...")
    pipeline.fit(X_train, y_train)
    print("âœ… ModÃ¨le entraÃ®nÃ©")
    return pipeline


def evaluate_model(pipeline: Pipeline, X_valid, y_valid):
    """Ã‰value le modÃ¨le et trouve le seuil optimal."""
    print("\nğŸ“ˆ Ã‰valuation sur l'ensemble de validation...")

    # PrÃ©dictions
    y_proba = pipeline.predict_proba(X_valid)[:, 1]

    # ROC-AUC
    roc_auc = roc_auc_score(y_valid, y_proba)

    # Precision-Recall curve
    precision, recall, thresholds = precision_recall_curve(y_valid, y_proba)
    pr_auc = auc(recall, precision)

    # Trouver le seuil optimal (Recall >= 85%, max Precision)
    target_recall = 0.85
    valid_indices = recall >= target_recall
    if valid_indices.any():
        best_idx = np.argmax(precision[valid_indices])
        optimal_threshold = thresholds[valid_indices][best_idx]
        optimal_precision = precision[valid_indices][best_idx]
        optimal_recall = recall[valid_indices][best_idx]
    else:
        optimal_threshold = 0.5
        optimal_precision = precision_score(y_valid, y_proba >= 0.5)
        optimal_recall = recall_score(y_valid, y_proba >= 0.5)

    metrics = {
        "roc_auc": float(roc_auc),
        "pr_auc": float(pr_auc),
        "threshold": float(optimal_threshold),
        "precision": float(optimal_precision),
        "recall": float(optimal_recall),
    }

    print(f"   ROC-AUC: {roc_auc:.4f}")
    print(f"   PR-AUC: {pr_auc:.4f}")
    print(f"   Seuil optimal: {optimal_threshold:.4f}")
    print(f"   Precision: {optimal_precision:.4f}")
    print(f"   Recall: {optimal_recall:.4f}")

    return metrics


def save_model(pipeline: Pipeline, metrics: dict, columns: list, output_dir: Path):
    """Sauvegarde le modÃ¨le et les artefacts."""
    print(f"\nğŸ’¾ Sauvegarde dans {output_dir}...")

    output_dir.mkdir(parents=True, exist_ok=True)

    # Sauvegarder le pipeline
    joblib.dump(pipeline, output_dir / "pipeline.joblib")

    # Sauvegarder les mÃ©triques
    with open(output_dir / "metrics_valid.json", "w") as f:
        json.dump(metrics, f, indent=2)

    # Sauvegarder les colonnes
    with open(output_dir / "columns.json", "w") as f:
        json.dump({"all_cols": columns}, f, indent=2)

    print("âœ… ModÃ¨le sauvegardÃ© avec succÃ¨s")


def main():
    """Fonction principale."""
    parser = argparse.ArgumentParser(description="EntraÃ®ner le modÃ¨le de dÃ©tection de fraude")
    parser.add_argument(
        "--data", type=str, required=True, help="Chemin vers les donnÃ©es CSV"
    )
    parser.add_argument(
        "--output", type=str, default="models/rf_smote_final", help="Dossier de sortie"
    )
    parser.add_argument(
        "--smote-strategy", type=float, default=0.2, help="StratÃ©gie SMOTE (default: 0.2)"
    )
    parser.add_argument(
        "--random-state", type=int, default=42, help="Random state (default: 42)"
    )

    args = parser.parse_args()

    # Chemins
    data_path = Path(args.data)
    output_dir = Path(args.output)

    if not data_path.exists():
        print(f"âŒ Erreur: {data_path} n'existe pas")
        sys.exit(1)

    print("=" * 70)
    print("ğŸ•µï¸  ENTRAÃNEMENT DU MODÃˆLE DE DÃ‰TECTION DE FRAUDE")
    print("=" * 70)

    # Pipeline complet
    df = load_data(data_path)
    X_train, X_valid, X_test, y_train, y_valid, y_test = split_data(df)
    pipeline = build_pipeline(smote_strategy=args.smote_strategy, random_state=args.random_state)
    pipeline = train_model(pipeline, X_train, y_train)
    metrics = evaluate_model(pipeline, X_valid, y_valid)
    save_model(pipeline, metrics, list(X_train.columns), output_dir)

    print("\n" + "=" * 70)
    print("âœ… ENTRAÃNEMENT TERMINÃ‰ AVEC SUCCÃˆS")
    print("=" * 70)
    print(f"\nğŸ“ ModÃ¨le disponible dans: {output_dir}")
    print(f"ğŸ¯ PR-AUC: {metrics['pr_auc']:.4f} | ROC-AUC: {metrics['roc_auc']:.4f}")


if __name__ == "__main__":
    main()
